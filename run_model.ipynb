{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T04:41:34.685304Z",
     "start_time": "2024-12-13T04:41:29.966135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import argparse\n",
    "import torch\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import Autoformer, DLinear, TimeLLM, TimeLLM_lora_bnb\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, vali, load_content\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Task parameters\n",
    "        self.task_name = 'long_term_forecast'  # options: [long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'test'\n",
    "        self.model_comment = 'none'\n",
    "        self.model = 'TimeLLM'  # options: [Autoformer, DLinear]\n",
    "        self.seed = 2021\n",
    "\n",
    "        # Data loader parameters\n",
    "        self.data = 'ETTm1'  # dataset type\n",
    "        self.root_path = \"../data/dataset/ETT-small\"  # root path of the data file\n",
    "        self.data_path = 'ETTm1.csv'  # data file\n",
    "        self.features = 'M'  # options: [M, S, MS]\n",
    "        self.target = 'OT'  # target feature in S or MS task\n",
    "        self.loader = 'modal'  # dataset type\n",
    "        self.freq = 'h'  # options: [s, t, h, d, b, w, m]\n",
    "        self.checkpoints = './checkpoints/'  # location of model checkpoints\n",
    "\n",
    "        # Forecasting task parameters\n",
    "        self.seq_len = 96  # input sequence length\n",
    "        self.label_len = 48  # start token length\n",
    "        self.pred_len = 96  # prediction sequence length\n",
    "        self.seasonal_patterns = 'Monthly'  # subset for M4\n",
    "\n",
    "        # Model definition parameters\n",
    "        self.enc_in = 7  # encoder input size\n",
    "        self.dec_in = 7  # decoder input size\n",
    "        self.c_out = 7   # output size\n",
    "        self.d_model = 16  # dimension of model\n",
    "        self.n_heads = 8   # num of heads\n",
    "        self.e_layers = 2   # num of encoder layers\n",
    "        self.d_layers = 1   # num of decoder layers\n",
    "        self.d_ff = 32      # dimension of fcn\n",
    "        self.moving_avg = 25   # window size of moving average\n",
    "        self.factor = 1      # attention factor\n",
    "        self.dropout = 0.1   # dropout rate\n",
    "        self.embed = 'timeF'   # time features encoding options: [timeF, fixed, learned]\n",
    "        self.activation = 'gelu'   # activation function\n",
    "        self.output_attention = False   # whether to output attention in encoder\n",
    "        self.patch_len = 16   # patch length\n",
    "        self.stride = 8       # stride length\n",
    "        self.prompt_domain = 0   # prompt domain (if applicable)\n",
    "        self.llm_model = 'LLAMA'   # LLM model options: [LLAMA, GPT2, BERT]\n",
    "        self.llm_dim = 4096    # LLM model dimension\n",
    "\n",
    "        # Optimization parameters\n",
    "        self.num_workers = 10   # data loader num workers\n",
    "        self.itr = 1            # experiments times\n",
    "        self.train_epochs = 10   # train epochs\n",
    "        self.align_epochs = 10    # alignment epochs\n",
    "        self.batch_size = 32     # batch size of train input data\n",
    "        self.eval_batch_size = 8   # batch size of model evaluation\n",
    "        self.patience = 10       # early stopping patience\n",
    "        self.learning_rate = 0.0001    # optimizer learning rate\n",
    "        self.des = 'test'       # experiment description\n",
    "        self.loss = 'MSE'       # loss function options: ['MSE', ...]\n",
    "        self.lradj = 'type1'    # adjust learning rate type options: ['type1', ...]\n",
    "        self.pct_start = 0.2     # pct_start for learning rate adjustment\n",
    "        self.use_amp = False      # use automatic mixed precision training\n",
    "        self.llm_layers = 6       # number of LLM layers\n",
    "        self.percent = 100\n",
    "        self.model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "\n",
    "# 使用示例：\n",
    "args = Args()\n",
    "print(args.task_name)           # 输出: long_term_forecast\n",
    "print(args.batch_size)\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "deepspeed_plugin = DeepSpeedPlugin(hf_ds_config='./ds_config_zero2.json')\n",
    "accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
    "setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}_{}'.format(\n",
    "    args.task_name,\n",
    "    args.model_id,\n",
    "    args.model,\n",
    "    args.data,\n",
    "    args.features,\n",
    "    args.seq_len,\n",
    "    args.label_len,\n",
    "    args.pred_len,\n",
    "    args.d_model,\n",
    "    args.n_heads,\n",
    "    args.e_layers,\n",
    "    args.d_layers,\n",
    "    args.d_ff,\n",
    "    args.factor,\n",
    "    args.embed,\n",
    "    args.des, 0)# 输出: 32"
   ],
   "id": "661b2c1b60846e0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_term_forecast\n",
      "32\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T04:42:09.824481Z",
     "start_time": "2024-12-13T04:41:34.747901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hf_token = \"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\"  #Put your own HF token here, do not publish it\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login directly with your Token (remember not to share this Token publicly)\n",
    "login(token=hf_token)\n",
    "import os\n",
    "model = TimeLLM_lora_bnb.Model(args).float()"
   ],
   "id": "eca68fa4aaeab228",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6fd0faae98de4fc2b9162d3d0a3b750b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-13T04:42:09.883014Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_data, train_loader = data_provider(args, 'train')\n",
    "vali_data, vali_loader = data_provider(args, 'val')\n",
    "test_data, test_loader = data_provider(args, 'test')\n",
    "early_stopping = EarlyStopping(accelerator=accelerator, patience=args.patience)\n",
    "path = os.path.join(args.checkpoints, setting + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "args.content = load_content(args)\n",
    "if not os.path.exists(path) and accelerator.is_local_main_process:\n",
    "    os.makedirs(path)\n",
    "time_now = time.time()\n",
    "train_steps = len(train_loader)\n",
    "model_optim = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "            steps_per_epoch=train_steps,\n",
    "            pct_start=args.pct_start,\n",
    "            epochs=args.train_epochs,\n",
    "            max_lr=args.learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "mae_metric = nn.L1Loss()\n",
    "\n",
    "# train_loader, vali_loader, test_loader, model, model_optim, scheduler = accelerator.prepare(\n",
    "#         train_loader, vali_loader, test_loader, model, model_optim, scheduler)\n",
    "iter_count = 0\n",
    "train_loss = []\n",
    "\n",
    "model.train()\n",
    "epoch_time = time.time()\n",
    "epoch = 0\n",
    "device = \"cpu\"\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(train_loader)):\n",
    "    iter_count += 1\n",
    "    model_optim.zero_grad()\n",
    "    print(f\"step {iter_count} 1\")\n",
    "\n",
    "    batch_x = batch_x.float().to(device)\n",
    "    batch_y = batch_y.float().to(device)\n",
    "    batch_x_mark = batch_x_mark.float().to(device)\n",
    "    batch_y_mark = batch_y_mark.float().to(device)\n",
    "    print(f\"step {iter_count} 2\")\n",
    "    # decoder input\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(\n",
    "        device)\n",
    "    dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(\n",
    "        device)\n",
    "    print(f\"step {iter_count} 3\")\n",
    "    if args.output_attention:\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)[0]\n",
    "    else:\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "    print(f\"step {iter_count} 4\")\n",
    "    f_dim = -1 if args.features == 'MS' else 0\n",
    "    outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "    batch_y = batch_y[:, -args.pred_len:, f_dim:]\n",
    "    loss = criterion(outputs, batch_y)\n",
    "    train_loss.append(loss.item())\n",
    "    print(f\"step {iter_count} 5\")\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\n",
    "            \"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "        speed = (time.time() - time_now) / iter_count\n",
    "        left_time = speed * ((args.train_epochs - epoch) * train_steps - i)\n",
    "        print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "        iter_count = 0\n",
    "        time_now = time.time()\n",
    "    loss.backward()\n",
    "    print(f\"step {iter_count} 6\")\n",
    "    model_optim.step()\n",
    "    print(f\"step {iter_count} 7\")\n",
    "    adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, args, printout=False)\n",
    "    print(f\"step {iter_count} 8\")\n",
    "    scheduler.step()\n",
    "    print(f\"step {iter_count} 9\")\n",
    "    print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "    train_loss = np.average(train_loss)\n",
    "    print(f\"step {iter_count} 10\")\n",
    "    vali_loss, vali_mae_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric)\n",
    "    print(f\"step {iter_count} 11\")\n",
    "    test_loss, test_mae_loss = vali(args, accelerator, model, test_data, test_loader, criterion, mae_metric)\n",
    "    print(f\"step {iter_count} 12\")\n",
    "    print(\n",
    "        \"Epoch: {0} | Train Loss: {1:.7f} Vali Loss: {2:.7f} Test Loss: {3:.7f} MAE Loss: {4:.7f}\".format(\n",
    "            epoch + 1, train_loss, vali_loss, test_loss, test_mae_loss))\n",
    "    early_stopping(vali_loss, model, path)\n",
    "    print(f\"step {iter_count} 13\")\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "    print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "    print(f\"step {iter_count} 14\")"
   ],
   "id": "e285794f2a67ff72",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 1\n",
      "step 1 2\n",
      "step 1 3\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:16:08.217351Z",
     "start_time": "2024-12-09T12:16:08.214199Z"
    }
   },
   "cell_type": "code",
   "source": "early_stopping",
   "id": "2d746dbd467d219",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<utils.tools.EarlyStopping at 0x7fa472f83910>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T12:22:53.867198Z",
     "start_time": "2024-12-09T12:22:53.861857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_parameters = []\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad is True:\n",
    "        trained_parameters.append(p)"
   ],
   "id": "8dcbce273c100113",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T02:37:44.629116Z",
     "start_time": "2024-12-10T02:37:44.616028Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5106f24eac05e1f1",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:28:24.110271Z",
     "start_time": "2024-12-10T03:28:24.099030Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "40302d4711a2f199",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T03:28:35.634786Z",
     "start_time": "2024-12-10T03:28:35.630933Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f4790a868db88e4c",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T13:20:04.869084Z",
     "start_time": "2024-12-10T13:20:04.857901Z"
    }
   },
   "cell_type": "code",
   "source": "device",
   "id": "4ead040c8e88984a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T14:10:04.168390Z",
     "start_time": "2024-12-10T14:09:13.110744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in tqdm(enumerate(train_loader)):\n",
    "    if i > 2: break\n",
    "    batch_x = batch_x.float().to(device)\n",
    "    batch_y = batch_y.float().to(device)\n",
    "    batch_x_mark = batch_x_mark.float().to(device)\n",
    "    batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "    print(batch_x.shape, batch_y.shape, batch_x_mark.shape, batch_y_mark.shape)"
   ],
   "id": "cf8f6218d47c3d5d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:01,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 96, 1]) torch.Size([32, 144, 1]) torch.Size([32, 96, 4]) torch.Size([32, 144, 4])\n",
      "torch.Size([32, 96, 1]) torch.Size([32, 144, 1]) torch.Size([32, 96, 4]) torch.Size([32, 144, 4])\n",
      "torch.Size([32, 96, 1]) torch.Size([32, 144, 1]) torch.Size([32, 96, 4]) torch.Size([32, 144, 4])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T03:13:08.284452Z",
     "start_time": "2024-12-11T03:13:08.279803Z"
    }
   },
   "cell_type": "code",
   "source": "args.label_len",
   "id": "47c9ea68e0275942",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T03:11:03.874168Z",
     "start_time": "2024-12-11T03:11:03.803550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :], dtype=torch.float32).float().to(\n",
    "                device)\n",
    "dec_inp = torch.cat([batch_y[:, :args.label_len, :].float().to(device), dec_inp], dim=1).to(\n",
    "    device)"
   ],
   "id": "4f80d1f533f58bb5",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T03:11:11.973622Z",
     "start_time": "2024-12-11T03:11:11.970463Z"
    }
   },
   "cell_type": "code",
   "source": "dec_inp.shape",
   "id": "e8c35a2c96d4df00",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 144, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "41669ebaddd86ee0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
