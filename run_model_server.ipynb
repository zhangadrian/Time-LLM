{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8f69396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniforge3/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/root/miniforge3/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "/root/miniforge3/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/root/miniforge3/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "long_term_forecast\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import Autoformer, DLinear, TimeLLM, TimeLLM_lora_bnb\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, vali, load_content\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        # Task parameters\n",
    "        self.task_name = 'long_term_forecast'  # options: [long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'test'\n",
    "        self.model_comment = 'none'\n",
    "        self.model = 'TimeLLM'  # options: [Autoformer, DLinear]\n",
    "        self.seed = 2021\n",
    "\n",
    "        # Data loader parameters\n",
    "        self.data = 'ETTm1'  # dataset type\n",
    "        self.root_path = \"../data/dataset/ETT-small\"  # root path of the data file\n",
    "        self.data_path = 'ETTm1.csv'  # data file\n",
    "        self.features = 'M'  # options: [M, S, MS]\n",
    "        self.target = 'OT'  # target feature in S or MS task\n",
    "        self.loader = 'modal'  # dataset type\n",
    "        self.freq = 'h'  # options: [s, t, h, d, b, w, m]\n",
    "        self.checkpoints = './checkpoints/'  # location of model checkpoints\n",
    "\n",
    "        # Forecasting task parameters\n",
    "        self.seq_len = 96  # input sequence length\n",
    "        self.label_len = 48  # start token length\n",
    "        self.pred_len = 96  # prediction sequence length\n",
    "        self.seasonal_patterns = 'Monthly'  # subset for M4\n",
    "\n",
    "        # Model definition parameters\n",
    "        self.enc_in = 7  # encoder input size\n",
    "        self.dec_in = 7  # decoder input size\n",
    "        self.c_out = 7   # output size\n",
    "        self.d_model = 16  # dimension of model\n",
    "        self.n_heads = 8   # num of heads\n",
    "        self.e_layers = 2   # num of encoder layers\n",
    "        self.d_layers = 1   # num of decoder layers\n",
    "        self.d_ff = 32      # dimension of fcn\n",
    "        self.moving_avg = 25   # window size of moving average\n",
    "        self.factor = 1      # attention factor\n",
    "        self.dropout = 0.1   # dropout rate\n",
    "        self.embed = 'timeF'   # time features encoding options: [timeF, fixed, learned]\n",
    "        self.activation = 'gelu'   # activation function\n",
    "        self.output_attention = False   # whether to output attention in encoder\n",
    "        self.patch_len = 16   # patch length\n",
    "        self.stride = 8       # stride length\n",
    "        self.prompt_domain = 0   # prompt domain (if applicable)\n",
    "        self.llm_model = 'LLAMA'   # LLM model options: [LLAMA, GPT2, BERT]\n",
    "        self.llm_dim = 4096    # LLM model dimension\n",
    "\n",
    "        # Optimization parameters\n",
    "        self.num_workers = 10   # data loader num workers\n",
    "        self.itr = 1            # experiments times\n",
    "        self.train_epochs = 10   # train epochs\n",
    "        self.align_epochs = 10    # alignment epochs\n",
    "        self.batch_size = 16     # batch size of train input data\n",
    "        self.eval_batch_size = 8   # batch size of model evaluation\n",
    "        self.patience = 10       # early stopping patience\n",
    "        self.learning_rate = 0.0001    # optimizer learning rate\n",
    "        self.des = 'test'       # experiment description\n",
    "        self.loss = 'MSE'       # loss function options: ['MSE', ...]\n",
    "        self.lradj = 'type1'    # adjust learning rate type options: ['type1', ...]\n",
    "        self.pct_start = 0.2     # pct_start for learning rate adjustment\n",
    "        self.use_amp = True      # use automatic mixed precision training\n",
    "        self.llm_layers = 6       # number of LLM layers\n",
    "        self.percent = 100\n",
    "        self.model_name = \"meta-llama/Llama-3.1-8B\"\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# 使用示例：\n",
    "args = Args()\n",
    "print(args.task_name)           # 输出: long_term_forecast\n",
    "print(args.batch_size)\n",
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "deepspeed_plugin = DeepSpeedPlugin(hf_ds_config='./ds_config_zero2.json')\n",
    "accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
    "setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}_{}'.format(\n",
    "    args.task_name,\n",
    "    args.model_id,\n",
    "    args.model,\n",
    "    args.data,\n",
    "    args.features,\n",
    "    args.seq_len,\n",
    "    args.label_len,\n",
    "    args.pred_len,\n",
    "    args.d_model,\n",
    "    args.n_heads,\n",
    "    args.e_layers,\n",
    "    args.d_layers,\n",
    "    args.d_ff,\n",
    "    args.factor,\n",
    "    args.embed,\n",
    "    args.des, 0)# 输出: 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94de161",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc8e0ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "181395f5-1cda-48fe-b709-3ae7599a4c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModel,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM\n",
    ")\n",
    "q_config = BitsAndBytesConfig(load_in_4bit=False,\n",
    "                                bnb_4bit_quant_type='nf4',\n",
    "                                bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.float16\n",
    "                                )\n",
    "from transformers.utils import is_bitsandbytes_available\n",
    "is_bitsandbytes_available()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd045c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca68fa4aaeab228",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T04:42:09.824481Z",
     "start_time": "2024-12-13T04:41:34.747901Z"
    }
   },
   "outputs": [],
   "source": [
    "hf_token = \"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\"  #Put your own HF token here, do not publish it\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login directly with your Token (remember not to share this Token publicly)\n",
    "login(token=hf_token)\n",
    "import os\n",
    "device = args.device\n",
    "model = TimeLLM_lora_bnb.Model(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "print(device)\n",
    "train_data, train_loader = data_provider(args, 'train')\n",
    "vali_data, vali_loader = data_provider(args, 'val')\n",
    "test_data, test_loader = data_provider(args, 'test')\n",
    "early_stopping = EarlyStopping(accelerator=accelerator, patience=args.patience)\n",
    "time_now = time.time()\n",
    "train_steps = len(train_loader)\n",
    "model_optim = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "scaler = GradScaler()\n",
    "scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                            steps_per_epoch=train_steps,\n",
    "                            pct_start=args.pct_start,\n",
    "                            epochs=args.train_epochs,\n",
    "                            max_lr=args.learning_rate)\n",
    "\n",
    "model.train()\n",
    "time_now = time.time()\n",
    "epoch = 0\n",
    "\n",
    "iter_count = 0\n",
    "train_loss = []\n",
    "\n",
    "time_cost_map = defaultdict(list)\n",
    "for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(tqdm(train_loader)):\n",
    "    model_optim.zero_grad()\n",
    "    batch_x = batch_x.float().to(device)\n",
    "    batch_y = batch_y.float().to(device)\n",
    "    batch_x_mark = batch_x_mark.float().to(device)\n",
    "    batch_y_mark = batch_y_mark.float().to(device)\n",
    "    dec_inp = torch.zeros_like(batch_y[:, -args.pred_len:, :]).float().to(device)\n",
    "    dec_inp = torch.cat([batch_y[:, :args.label_len, :], dec_inp], dim=1).float().to(device)\n",
    "\n",
    "    with autocast():  # autocast context manager\n",
    "        outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "        start_time = time.time()\n",
    "        f_dim = -1 if args.features == 'MS' else 0\n",
    "        outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "        batch_y = batch_y[:, -args.pred_len:, f_dim:]\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "    \n",
    "    scaler.scale(loss).backward()  # scale the loss\n",
    "    scaler.step(model_optim)  # update parameters\n",
    "    scaler.update()\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"\\titers: {i + 1}, epoch: {epoch + 1} | loss: {loss.item():.7f}\")\n",
    "        speed = (time.time() - time_now) / iter_count\n",
    "        left_time = speed * ((args.train_epochs - epoch) * train_steps - i)\n",
    "        print(f'\\tspeed: {speed:.4f}s/iter; left time: {left_time:.4f}s')\n",
    "        iter_count = 0\n",
    "        time_now = time.time()\n",
    "    \n",
    "    adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, args, printout=False)\n",
    "    scheduler.step()\n",
    "    iter_count += 1\n",
    "train_loss = np.average(train_loss)\n",
    "\n",
    "start_time = time.time()\n",
    "vali_loss, vali_mae_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric)\n",
    "time_cost_map[\"Validation\"].append(f\"{time.time() - start_time:.4f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss, test_mae_loss = vali(args, accelerator, model, test_data, test_loader, criterion, mae_metric)\n",
    "time_cost_map[\"Test\"].append(f\"{time.time() - start_time:.4f}s\")\n",
    "\n",
    "print(f\"Epoch: {epoch + 1} | Train Loss: {train_loss:.7f} Vali Loss: {vali_loss:.7f} Test Loss: {test_loss:.7f} MAE Loss: {test_mae_loss:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961e5c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/root/FinAI/Time-LLM/checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a91265",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.use_amp = True\n",
    "mae_metric = nn.L1Loss()\n",
    "train_loss = np.average(train_loss)\n",
    "print(train_loss)\n",
    "start_time = time.time()\n",
    "vali_loss, vali_mae_loss = vali(args, accelerator, model, vali_data, vali_loader, criterion, mae_metric)\n",
    "time_cost_map[\"Validation\"].append(f\"{time.time() - start_time:.4f}s\")\n",
    "\n",
    "start_time = time.time()\n",
    "test_loss, test_mae_loss = vali(args, accelerator, model, test_data, test_loader, criterion, mae_metric)\n",
    "time_cost_map[\"Test\"].append(f\"{time.time() - start_time:.4f}s\")\n",
    "\n",
    "print(f\"Epoch: {epoch + 1} | Train Loss: {train_loss:.7f} Vali Loss: {vali_loss:.7f} Test Loss: {test_loss:.7f} MAE Loss: {test_mae_loss:.7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0479b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e6206f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
